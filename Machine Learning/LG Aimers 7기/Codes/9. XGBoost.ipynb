{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["eXr9jjHAM9XA","TJOjvPe9ksPw","uZpGTVddnjLs","A2T2914Lliv1"],"mount_file_id":"1B9KZURBMorTu5s3g9Bu07T1kHGd__aiW","authorship_tag":"ABX9TyOaPw/BUU4gy5auV0bIFjEI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ul9godsJEILo"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from pathlib import Path\n","\n","import datetime\n","from datetime import timedelta\n","# !pip install holidays\n","import holidays\n","\n","os.chdir(\"/content/drive/MyDrive/3. Grad School/LG Aimers\")"]},{"cell_type":"code","source":["# 데이터 불러오기\n","data = pd.read_csv(\"DATA/train/train.csv\")"],"metadata":{"id":"3iZ_a2k0McT1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 파생변수 생성"],"metadata":{"id":"bx02HI7OMeST"}},{"cell_type":"code","source":["# 월(month) -> 계절 매핑 딕셔너리\n","month_to_season = {\n","    1: \"Winter\", 2: \"Winter\", 12: \"Winter\",\n","    3: \"Spring\", 4: \"Spring\", 5: \"Spring\",\n","    6: \"Summer\", 7: \"Summer\", 8: \"Summer\",\n","    9: \"Autumn\", 10: \"Autumn\", 11: \"Autumn\"}\n","\n","season_weights = {\n","    \"Winter\" : 11.4,\n","    \"Spring\": 6.5,\n","    \"Summer\" : 6.3,\n","    \"Autumn\" : 18.4}\n","\n","# 월별 가중치 매핑\n","monthly_weights = {\n","    1: 2.2, 2: 1.8, 3: 0.3,\n","    4: 1.01, 5: 0.7, 6: 0.8,\n","    7: 0.5, 8: 0.5, 9: 0.8,\n","    10: 1.55, 11: 1.03, 12: 1.4}\n","\n","# 요일별 가중치 매핑\n","weekly_weights = {\n","    \"Monday\": 0.78, \"Tuesday\": 0.85, \"Wednesday\": 0.81,\n","    \"Thursday\": 9.9, \"Friday\": 1.2, \"Saturday\": 1.53,\n","    \"Sunday\": 1.3}"],"metadata":{"id":"PT4DKLKAMgBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Make_Variables():\n","        def __init__(self, data = None, date = None, predict = 7, month_to_season = None, monthly_weights = None, weekly_weights = None):\n","            self.data = data\n","            self.date = date\n","            self.predict = predict\n","            self.month_to_season = month_to_season\n","            self.monthly_weights = monthly_weights\n","            self.weekly_weights = weekly_weights\n","\n","        def update_kor_holidays(self):\n","            \"\"\"국경일 추가\"\"\"\n","            kor_holidays = holidays.KR(years = [2023, 2024, 2025])\n","            kor_holidays.update({\n","                datetime.date(2024,10,1) : \"Temporary Holiday\", # 국군의 날 임시공휴일\n","                datetime.date(2025,1,27) : \"Temporary Holiday\", # 설날 임시공휴일\n","                datetime.date(2025,3,3) : \"Temporary Holiday\", # 삼일절 대체공휴일\n","                datetime.date(2025, 6, 3) : \"Presidential Election Day\"})\n","            return kor_holidays\n","\n","        def check_holidays(self, date, kor_holidays) -> int:\n","            \"\"\"날짜 받아서 공휴일/주말 여부 출력\"\"\"\n","            # date = pd.Timestamp(date)\n","            if isinstance(date, pd.Series):\n","                check_holiday = date.dt.date.isin(kor_holidays)\n","                check_weekend = date.dt.weekday >= 5\n","            else:\n","                check_holiday = date.date() in kor_holidays\n","                check_weekend = date.weekday() >= 5\n","            is_holiday = (check_holiday | check_weekend)\n","            return is_holiday\n","\n","        def get_sandwich_score(self, data, is_holiday_col) -> pd.DataFrame:\n","            \"\"\"데이터프레임 기준으로 샌드위치 점수 계산\"\"\"\n","            data = data.reset_index(drop = True)\n","            data['is_sandwich'] = 0\n","            is_holiday = data[is_holiday_col].astype(int)\n","            for idx in range(len(data)):\n","                if idx == 0 or idx == len(data) - 1: # 첫날, 마지막 날\n","                    continue\n","\n","                # 앞/뒤 하루씩 봤을 때 모두 휴일 -> 5점\n","                if (is_holiday.iloc[idx - 1] == 1) and (is_holiday.iloc[idx + 1] == 1): # 하루 전이랑 다음 날이 공휴일이면\n","                    data.iloc[idx, data.columns.get_loc('is_sandwich')] = 5\n","\n","                # 앞/뒤 이틀씩 봤을 때 휴일 3일 -> 3점, 2일 -> 2점\n","                elif idx > 1 and idx < len(is_holiday) - 2: # 셋째날, 마지막에서 세 번째 날\n","                    start_idx = idx - 2\n","                    end_idx = idx + 2\n","                    nearby_holidays = (is_holiday.iloc[start_idx : end_idx + 1].sum() - is_holiday.iloc[idx])\n","                    if nearby_holidays == 3:\n","                        data.iloc[idx, data.columns.get_loc('is_sandwich')] = 3\n","                    elif nearby_holidays == 2:\n","                        data.iloc[idx, data.columns.get_loc('is_sandwich')] = 2\n","                    else:\n","                        data.iloc[idx, data.columns.get_loc('is_sandwich')] = 0\n","            return data\n","\n","        def get_sandwich_score_for_dates(self, date, kor_holidays) -> int:\n","            \"\"\"특정 날짜를 받아와서 앞뒤 날짜를 구하고, 샌드위치 점수 계산\"\"\"\n","            # 하루씩\n","            prev_date, next_date = date - timedelta(days = 1), date + timedelta(days = 1)\n","            prev_hol, next_hol = self.check_holidays(prev_date, kor_holidays), self.check_holidays(next_date, kor_holidays) # T/F Bool\n","            if prev_hol and next_hol: # 바로 다음 날들이 휴일이라면\n","                return 5\n","            days_offsets = [-2, -1, 1, 2] # 앞뒤로 이틀 살펴보기\n","            nearby_holidays = sum(self.check_holidays(date + timedelta(days = d), kor_holidays) for d in days_offsets)\n","            if nearby_holidays == 3: # 앞뒤 4일 중에 3일이 휴일이면\n","                return 3\n","            elif nearby_holidays == 2: # 앞뒤 4일 중에 2일이 휴일이면\n","                return 2\n","            else:\n","                return 0\n","\n","        def get_season_weights(self, data = None, season_weights = season_weights):\n","            \"\"\"계절별 가중치 부여\"\"\"\n","            # 데이터프레임 들어오면\n","            if data is not None:\n","                data['season_weight'] = data['season'].map(season_weights)\n","                return data\n","\n","        def get_month_weights(self, data = None, monthly_weights = monthly_weights):\n","            \"\"\"월별 가중치 부여\"\"\"\n","            # 데이터프레임 들어오면\n","            if data is not None:\n","                data['month_weight'] = data['month'].map(monthly_weights)\n","                return data\n","\n","        def get_week_weights(self, data = None, weekly_weights = weekly_weights):\n","            \"\"\"요일별 가중치 부여\"\"\"\n","            # 데이터프레임 들어오면\n","            if data is not None:\n","                data['week_weight'] = data['weekday'].map(weekly_weights)\n","                return data\n","\n","        def get_prev_days(self, data, test_df = None, date = None, menu = None, howmany = 7):\n","            \"\"\"\n","            일요일 날짜 받아와서 직전 주차의 일-토 매출수량 평균 계산\n","            주의 - test data에서 생성할 때는 참고할 데이터와 붙여넣을 데이터가 다름\n","            data : 참고할 데이터\n","            test_df : 참고할 데이터\n","            \"\"\"\n","            if test_df is None:\n","                # 혹시 모르니까 검증\n","                if date.weekday() == 6:\n","                    # 이전 날짜들\n","                    prev_start = date - timedelta(days = howmany)\n","                    prev_end = date - timedelta(days = 1)\n","                    prev_data = data[(data['영업일자'] >= prev_start) & (data['영업일자'] <= prev_end) & (data['영업장명_메뉴명'] == menu)]\n","                    prev_avg = prev_data['매출수량'].mean()\n","                    prev_sd = prev_data['매출수량'].std()\n","                    # 첫 주 0으로 처리\n","                    if pd.isna(prev_avg):\n","                        prev_avg = 0\n","                    if pd.isna(prev_sd):\n","                        prev_sd = 0\n","                    week_end = date + timedelta(days = 6)\n","                    curr_mask = (data['영업일자'] >= date) & (data['영업일자'] <= week_end) & (data['영업장명_메뉴명'] == menu)\n","                    colname_mean = f\"prev_avg_{howmany}\"\n","                    colname_sd = f\"prev_sd_{howmany}\"\n","                    data.loc[curr_mask, colname_mean] = prev_avg\n","                    data.loc[curr_mask, colname_sd] = prev_sd\n","                    return data\n","                else:\n","                    return np.nan\n","\n","            # test data라면\n","            else:\n","                # 혹시 모르니까 검증\n","                if date.weekday() == 6:\n","                    # 이전 날짜들\n","                    prev_start = date - timedelta(days = howmany)\n","                    prev_end = date - timedelta(days = 1)\n","                    prev_data = test_df[(test_df['영업일자'] >= prev_start) & (test_df['영업일자'] <= prev_end) & (test_df['영업장명_메뉴명'] == menu)]\n","                    prev_avg = prev_data['매출수량'].mean()\n","                    prev_sd = prev_data['매출수량'].std()\n","                    # 첫 주 0으로 처리\n","                    if pd.isna(prev_avg):\n","                        prev_avg = 0\n","                    if pd.isna(prev_sd):\n","                        prev_sd = 0\n","                    week_end = date + timedelta(days = 6)\n","                    curr_mask = (data['영업일자'] >= date) & (data['영업일자'] <= week_end) & (data['영업장명_메뉴명'] == menu)\n","                    colname_mean = f\"prev_avg_{howmany}\"\n","                    colname_sd = f\"prev_sd_{howmany}\"\n","                    data.loc[curr_mask, colname_mean] = prev_avg\n","                    data.loc[curr_mask, colname_sd] = prev_sd\n","                    return data\n","                else:\n","                    return np.nan\n","\n","        def get_means(self, data, original_data = None):\n","            # train 단계일 때\n","            if original_data is None:\n","                store_avg = data.groupby(\"영업장명_메뉴명\")['매출수량'].mean()\n","                menu_avg = data.groupby(\"영업장명\")['매출수량'].mean()\n","                return store_avg, menu_avg\n","            else:\n","                merged = pd.concat([data, original_data], axis = 0, ignore_index = True)\n","\n","                store_avg = merged.groupby(\"영업장명_메뉴명\")['매출수량'].mean()\n","                menu_avg = merged.groupby(\"영업장명\")['매출수량'].mean()\n","                return store_avg, menu_avg\n","\n","        def get_seasonal(self, data, original_data = None):\n","            if original_data is None:\n","                # 영업장별\n","                store_season = (data.groupby(['영업장명', 'season'])['매출수량'].mean().reset_index().rename(columns = {'매출수량' : 'store_season'}))\n","                data = data.merge(store_season, on = ['영업장명', 'season'], how = 'left')\n","                data['store_season_ratio'] = data['store_season'] / data['store_avg']\n","\n","                # 메뉴별\n","                menu_season = (data.groupby(['영업장명_메뉴명', 'season'])['매출수량'].mean().reset_index().rename(columns = {'매출수량' : 'menu_season'}))\n","                data = data.merge(menu_season, on = ['영업장명_메뉴명', 'season'], how = 'left')\n","                data['menu_season_ratio'] = data['menu_season'] / data['menu_avg']\n","\n","                return data\n","            else:\n","                store_season = (original_data.groupby(['영업장명', 'season'])['매출수량'].mean().reset_index().rename(columns = {'매출수량' : 'store_season'}))\n","                data = data.merge(store_season, on = ['영업장명', 'season'], how = 'left')\n","                data['store_season_ratio'] = data['store_season'] / data['store_avg']\n","\n","                # 메뉴별\n","                menu_season = (original_data.groupby(['영업장명_메뉴명', 'season'])['매출수량'].mean().reset_index().rename(columns = {'매출수량' : 'menu_season'}))\n","                data = data.merge(menu_season, on = ['영업장명_메뉴명', 'season'], how = 'left')\n","                data['menu_season_ratio'] = data['menu_season'] / data['menu_avg']\n","\n","                return data\n","\n","        # train, test 공통\n","        def make_fund_variables(self, data, month_to_season = month_to_season):\n","            # 영업일자 -> datetime\n","            data['영업일자'] = pd.to_datetime(data['영업일자'])\n","\n","            # 연, 월, 일, 요일 분리\n","            data['year'] = data['영업일자'].dt.year\n","            data['month'] = data['영업일자'].dt.month\n","            data['day'] = data['영업일자'].dt.day\n","            data['weekday'] = data['영업일자'].dt.day_name()\n","            data['weekday_enc'] = data['영업일자'].dt.weekday\n","\n","            # 계절 변수 생성\n","            data['season'] = data['month'].map(month_to_season)\n","\n","            # 연도 차이 변수 생성\n","            data['year_enc'] = data['year'] - 2023\n","\n","            # 월, 일, 요일 사이클릭 변환\n","            data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n","            data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n","\n","            data['day_sin'] = np.sin(2 * np.pi * data['day'] / 31)\n","            data['day_cos'] = np.cos(2 * np.pi * data['day'] / 31)\n","\n","            data['weekday_sin'] = np.sin(2 * np.pi * data['weekday_enc'] / 7)\n","            data['weekday_cos'] = np.cos(2 * np.pi * data['weekday_enc'] / 7)\n","\n","            # 공휴일 확인\n","            kor_holidays = self.update_kor_holidays()\n","            check_holiday = data['영업일자'].dt.date.isin(kor_holidays)\n","            check_weekend = data['weekday'].isin(['Saturday', 'Sunday'])\n","            data['is_holiday'] = (check_holiday | check_weekend).astype(int) # 공휴일 + 주말\n","            data['holiday_name'] = data['영업일자'].dt.date.map(kor_holidays)\n","\n","            ### 영업장명, 메뉴명 분리\n","            if '영업장명_메뉴명' in data.columns:\n","                data[['영업장명', '메뉴명']] = data['영업장명_메뉴명'].str.split('_', expand = True)\n","\n","            ### 단체 변수\n","            group_words = [\"단체\", \"6인석\", \"12인석\", \"2인\", \"4인\", \"3인\"]\n","            mask = data['메뉴명'].astype(str).apply(lambda x : any(k in x for k in group_words))\n","            data['group'] = mask.astype(int)\n","\n","            return data\n","\n","        # train의 입력 데이터 + lookback 28일 데이터\n","        def make_variables_train(self, data):\n","            data = self.make_fund_variables(data)\n","            kor_holidays = self.update_kor_holidays()\n","\n","            ### 샌드위치 데이\n","            data = self.get_sandwich_score(data, 'is_holiday')\n","\n","            # 샌드위치 - 첫날\n","            first = data['영업일자'].min()\n","            data.loc[data['영업일자'] == first, 'is_sandwich'] = self.get_sandwich_score_for_dates(first, kor_holidays)\n","            second = data['영업일자'].min() + timedelta(days = 1)\n","            data.loc[data['영업일자'] == second, 'is_sandwich'] = self.get_sandwich_score_for_dates(second, kor_holidays)\n","\n","            # 샌드위치 - 마지막 날\n","            last = data['영업일자'].max()\n","            data.loc[data['영업일자'] == last, 'is_sandwich'] = self.get_sandwich_score_for_dates(last, kor_holidays)\n","            before = data['영업일자'].max() - timedelta(days = 1)\n","            data.loc[data['영업일자'] == before, 'is_sandwich'] = self.get_sandwich_score_for_dates(before, kor_holidays)\n","\n","            # 서브웨이 샌드위치 포함한 공휴일\n","            data['is_holiday_sandwich'] = data['is_holiday'].astype(int) | (data['is_sandwich'] > 0).astype(int)\n","\n","            ### 계절별 가중치\n","            data = self.get_season_weights(data, season_weights)\n","\n","            ### 월별 가중치\n","            data = self.get_month_weights(data, monthly_weights)\n","\n","            ### 요일별 가중치\n","            data = self.get_week_weights(data, weekly_weights)\n","\n","            ### 직전 주차 평균\n","            sundays = data[data['weekday'] == \"Sunday\"][[\"영업일자\", \"영업장명_메뉴명\"]].copy()\n","            for _, row in sundays.iterrows():\n","                date = row['영업일자']\n","                menu = row['영업장명_메뉴명']\n","                data = self.get_prev_days(data = data, date = date, menu = menu, howmany = 7)\n","                data = self.get_prev_days(data = data, date = date, menu = menu, howmany = 14)\n","                data = self.get_prev_days(data = data, date = date, menu = menu, howmany = 21)\n","\n","            ### 음수 처리\n","            negative = data[data['매출수량'] < 0]\n","\n","            for idx, row in negative.iterrows():\n","                num = row['매출수량']\n","                if num < -10:\n","                    date = row['영업일자']\n","                    menu = row['영업장명_메뉴명']\n","                    prev_date = pd.to_datetime(date) - pd.Timedelta(days = 1)\n","                    prev_row = data[(data['영업일자'] == prev_date) & (data['영업장명_메뉴명'] == menu)]\n","\n","                    if prev_row.iloc[0][\"매출수량\"] >= abs(num):\n","                        data.loc[prev_row.index[0], '매출수량'] += num\n","\n","            # 남은 건 전부 0으로\n","            data.loc[data['매출수량'] < 0, '매출수량'] = 0\n","\n","            # 평균 매핑 - data에 test_df 들어가고, original_data에 기존 train에 사용한 데이터 넣기\n","            store_avg, menu_avg = self.get_means(data, original_data = None)\n","            data['store_avg'] = data['영업장명_메뉴명'].map(store_avg)\n","            data['menu_avg'] = data['영업장명'].map(menu_avg)\n","\n","            ### 계절 영향 추가\n","            data = self.get_seasonal(data)\n","\n","            return data\n","\n","        # 예측하고자 하는 날들\n","        def make_variables_test(self, date, test_df, original_data = None, predict = 7):\n","            \"\"\"\n","            date : 최종 날짜 (입력 7일 중 가장 마지막) - TimeStamp\n","            test_df : 예측할 때 참고해올 데이터 -> 이거로 직전 주차 평균 생성\n","            original_data -> train에 사용한 데이터 (data로 저장)\n","            \"\"\"\n","            date = pd.to_datetime(date)\n","            future_dates = [date + timedelta(days = i + 1) for i in range(predict)]\n","            future_df = pd.DataFrame({'영업일자' : future_dates})\n","\n","            menus_df = (test_df[['영업장명_메뉴명']].drop_duplicates().reset_index(drop = True))\n","            future_df = future_df.merge(menus_df, how='cross')\n","\n","            kor_holidays = self.update_kor_holidays()\n","\n","            # 기본적인 변수들\n","            future_df = self.make_fund_variables(future_df)\n","\n","            future_df['영업일자'] = pd.to_datetime(future_df['영업일자']).dt.normalize()\n","\n","            # 샌드위치\n","            future_df['is_sandwich'] = future_df['영업일자'].apply(lambda d: self.get_sandwich_score_for_dates(d, kor_holidays))\n","\n","            # 샌드위치 포함한 공휴일\n","            future_df['is_holiday_sandwich'] = future_df['is_holiday'].astype(int) | (future_df['is_sandwich'] > 0).astype(int)\n","\n","            ### 계절별 가중치\n","            future_df = self.get_season_weights(future_df, season_weights)\n","\n","            # 월별 가중치\n","            future_df = self.get_month_weights(future_df, monthly_weights)\n","\n","            # 요일별 가중치\n","            future_df = self.get_week_weights(future_df, weekly_weights)\n","\n","            # 직전 주차 평균 -> 이거는 test 까지 받아오고 생각해야 함..\n","            sundays =  future_df.loc[future_df['weekday'] == \"Sunday\", ['영업일자', '영업장명_메뉴명']].copy()\n","            for _, row in sundays.iterrows():\n","                date = row['영업일자']\n","                menu = row['영업장명_메뉴명']\n","                future_df = self.get_prev_days(data = future_df, test_df = test_df, date = date, menu = menu, howmany = 7)\n","                future_df = self.get_prev_days(data = future_df, test_df = test_df, date = date, menu = menu, howmany = 14)\n","                future_df = self.get_prev_days(data = future_df, test_df = test_df, date = date, menu = menu, howmany = 21)\n","\n","            # 앞서 계산한 값들로 매핑\n","            store_avg, menu_avg = self.get_means(future_df, original_data = original_data)\n","            future_df['store_avg'] = future_df['영업장명_메뉴명'].map(store_avg)\n","            future_df['menu_avg'] = future_df['영업장명'].map(menu_avg)\n","\n","            ### 계절 영향 추가\n","            future_df = self.get_seasonal(future_df, original_data = original_data)\n","\n","            return future_df"],"metadata":{"id":"P18JkDQeMiP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 그냥 전부 만들면 돼\n","mv = Make_Variables()\n","data = mv.make_variables_train(data = data)"],"metadata":{"id":"sruGvjBKMo2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6RaG41uBPNL","executionInfo":{"status":"ok","timestamp":1756054768188,"user_tz":-540,"elapsed":30,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"1335ea2b-8181-4e23-c055-8a0a5003b926"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['영업일자', '영업장명_메뉴명', '매출수량', 'year', 'month', 'day', 'weekday',\n","       'weekday_enc', 'season', 'year_enc', 'month_sin', 'month_cos',\n","       'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos', 'is_holiday',\n","       'holiday_name', '영업장명', '메뉴명', 'group', 'is_sandwich',\n","       'is_holiday_sandwich', 'season_weight', 'month_weight', 'week_weight',\n","       'prev_avg_7', 'prev_sd_7', 'prev_avg_14', 'prev_sd_14', 'prev_avg_21',\n","       'prev_sd_21', 'store_avg', 'menu_avg', 'store_season',\n","       'store_season_ratio', 'menu_season', 'menu_season_ratio'],\n","      dtype='object')"]},"metadata":{},"execution_count":136}]},{"cell_type":"code","source":["import pickle\n","data.to_pickle(\"/content/drive/MyDrive/3. Grad School/LG Aimers/DATA/train_data_final.pickle\")"],"metadata":{"id":"yR85L5x0M5nD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 저장된 데이터 불러오기"],"metadata":{"id":"eXr9jjHAM9XA"}},{"cell_type":"code","source":["import pickle\n","data = pd.read_pickle(\"/content/drive/MyDrive/3. Grad School/LG Aimers/DATA/train_data_final.pickle\") ################################"],"metadata":{"id":"N9PpyT-QM_kN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols =  [\"year_enc\", \"month_sin\", \"month_cos\", \"day_sin\", \"day_cos\", \"weekday_sin\", \"weekday_cos\",\n","         \"season\", \"is_holiday\", \"is_sandwich\", \"is_holiday_sandwich\", \"season_weight\", \"month_weight\", \"week_weight\",\n","         \"prev_avg_7\", \"prev_avg_14\", \"prev_avg_21\", \"prev_sd_7\", \"prev_sd_14\", \"prev_sd_21\",\n","         \"영업장명\", \"메뉴명\", \"store_avg\", \"menu_avg\"]\n","\n","cols = cols + ['store_season', 'store_season_ratio', 'menu_season', 'menu_season_ratio']\n","\n","enc_cols = [\"season\", \"영업장명\", \"메뉴명\"]"],"metadata":{"id":"685hlJjONCWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### XGBoost"],"metadata":{"id":"PZAG1lc8DV-e"}},{"cell_type":"code","source":["df_processed = data\n","df_processed.fillna(0, inplace=True)\n","print(\"기본 피처 생성 완료\")\n","\n","# 추가 피처 생성 (is_signature, is_essential)\n","# is_signature 생성\n","sales_sum = df_processed.groupby('영업장명_메뉴명')['매출수량'].sum()\n","signature_menus = sales_sum[sales_sum >= 10000].index.tolist()\n","df_processed['is_signature'] = df_processed['영업장명_메뉴명'].isin(signature_menus).astype(int)\n","print(f\"시그니처 메뉴 피처 생성 완료: 총 {len(signature_menus)}개\")\n","\n","# is_essential 생성\n","damha_non_essential = [\n","    '(후식) 된장찌개', '메밀면 사리', '참이슬', '카스', '(후식) 물냉면', '테라',\n","    '느린마을 막걸리', '콜라', '스프라이트', '라면사리', '갱시기',\n","    '(후식) 비빔냉면', '처음처럼', '더덕 한우 지짐', '하동 매실 칵테일',\n","    '제로콜라', '(단체) 공깃밥', '룸 이용료', '문막 복분자 칵테일', '명인안동소주'\n","]\n","mirasia_non_essential = [\n","    '버드와이저(무제한)', '애플망고 에이드','스텔라(무제한)', '코카콜라',\n","    '핑크레몬에이드', '얼그레이 하이볼', 'BBQ 고기추가', '스프라이트',\n","    '유자 하이볼', '코카콜라(제로)', '콥 샐러드', '칠리 치즈 프라이',\n","    '레인보우칵테일(알코올)', '파스타면 추가(150g)', '공깃밥',\n","    '글라스와인 (레드)', '잭 애플 토닉'\n","]\n","banquet_non_essential = [\n","   '삼겹살추가 (200g)', 'Convention Hall', '마라샹궈', 'Grand Ballroom', '야채추가',\n","    'Conference L1', 'Conference M1', 'Conference L3', 'Conference L2',\n","    'OPUS 2', 'Conference M9', 'Conference M8'\n","]\n","\n","def create_is_essential(row):\n","    store = row['영업장명']\n","    menu = row['메뉴명']\n","\n","    if store == '담하':\n","        return 0 if menu in damha_non_essential else 1\n","    elif store == '미라시아':\n","        return 0 if menu in mirasia_non_essential else 1\n","    elif store == '연회장':\n","        # 연회장 메뉴 이름에서 콜론 뒷부분을 제거하고 비교\n","        cleaned_menu = menu.split(':')[0].strip()\n","        return 0 if cleaned_menu in banquet_non_essential else 1\n","    elif store == '카페테리아':\n","        return 0 if menu == '샷 추가' else 1\n","    else:\n","        return 1\n","\n","df_processed['is_essential'] = df_processed.apply(create_is_essential, axis=1)\n","print(\"--- 'is_essential' 피처 생성 규칙 검증 ---\")\n","\n","\n","# --- 검증 로직 ---\n","expected_zero_counts = {\n","    '담하': len(damha_non_essential),\n","    '미라시아': len(mirasia_non_essential),\n","    '연회장': len(banquet_non_essential),\n","    '카페테리아': 1,\n","}\n","\n","results = []\n","all_stores = sorted(df_processed['영업장명'].unique())\n","\n","for store_name in all_stores:\n","    store_df = df_processed[df_processed['영업장명'] == store_name]\n","    total_menus_in_data = store_df['메뉴명'].nunique()\n","    actual_zero_count = store_df[store_df['is_essential'] == 0]['메뉴명'].nunique()\n","    actual_one_count = store_df[store_df['is_essential'] == 1]['메뉴명'].nunique()\n","    expected_zeros = expected_zero_counts.get(store_name, 0)\n","    is_correct = (actual_zero_count == expected_zeros)\n","\n","    results.append({\n","        '영업장명': store_name,\n","        '총 메뉴 수': total_menus_in_data,\n","        \"'제외 메뉴'(0) 실제 개수\": actual_zero_count,\n","        \"'제외 메뉴'(0) 예상 개수\": expected_zeros,\n","        \"'필수 메뉴'(1) 실제 개수\": actual_one_count,\n","        '규칙 일치 여부': ' 일치' if is_correct else ' 불일치'\n","    })\n","\n","verification_df = pd.DataFrame(results)\n","print(verification_df.to_string())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSuyI6jwDVZK","executionInfo":{"status":"ok","timestamp":1756083012957,"user_tz":-540,"elapsed":2141,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"360b8dec-77f3-47ce-aecf-df9211602866"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["기본 피처 생성 완료\n","시그니처 메뉴 피처 생성 완료: 총 40개\n","--- 'is_essential' 피처 생성 규칙 검증 ---\n","         영업장명  총 메뉴 수  '제외 메뉴'(0) 실제 개수  '제외 메뉴'(0) 예상 개수  '필수 메뉴'(1) 실제 개수 규칙 일치 여부\n","0  느티나무 셀프BBQ      23                 0                 0                23       일치\n","1          담하      42                20                20                22       일치\n","2        라그로타      25                 0                 0                25       일치\n","3        미라시아      31                17                17                14       일치\n","4         연회장      23                12                12                11       일치\n","5       카페테리아      24                 1                 1                23       일치\n","6       포레스트릿      12                 0                 0                12       일치\n","7       화담숲주막       8                 0                 0                 8       일치\n","8       화담숲카페       5                 0                 0                 5       일치\n"]}]},{"cell_type":"code","source":["signature_df = df_processed[df_processed['is_signature'] == 1].copy()\n","\n","# 영업장명으로 그룹화한 뒤, 각 그룹의 고유한 메뉴명 개수를 계산\n","signature_counts_by_store = signature_df.groupby('영업장명')['메뉴명'].nunique()\n","\n","# 결과 출력\n","print(\"--- 영업장별 시그니처 메뉴 개수 ---\")\n","print(signature_counts_by_store)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ns110kwkD0fv","executionInfo":{"status":"ok","timestamp":1756083014269,"user_tz":-540,"elapsed":24,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"63a5ed1a-463a-4d1b-becb-1f6a6bd43ec1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 영업장별 시그니처 메뉴 개수 ---\n","영업장명\n","느티나무 셀프BBQ     3\n","담하             3\n","미라시아           4\n","카페테리아         10\n","포레스트릿         10\n","화담숲주막          6\n","화담숲카페          4\n","Name: 메뉴명, dtype: int64\n"]}]},{"cell_type":"code","source":["signature_menus_by_store = signature_df.groupby('영업장명')['메뉴명'].apply(lambda x: x.unique().tolist())\n","print(\"--- 영업장별 시그니처 메뉴 목록 ---\")\n","print(signature_menus_by_store)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qyea148nD4jj","executionInfo":{"status":"ok","timestamp":1756083017100,"user_tz":-540,"elapsed":32,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"e873a956-a0fa-4000-b6e1-4d57bebb9413"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 영업장별 시그니처 메뉴 목록 ---\n","영업장명\n","느티나무 셀프BBQ                      [BBQ55(단체), 참이슬 (단체), 카스 병(단체)]\n","담하                          [(단체) 황태해장국 3/27까지, 공깃밥, 담하 한우 불고기]\n","미라시아          [(단체)브런치주중 36,000, 미라시아 브런치 (패키지), 브런치(대인) 주말,...\n","카페테리아         [공깃밥(추가), 단체식 13000(신), 단체식 18000(신), 돼지고기 김치찌...\n","포레스트릿         [꼬치어묵, 떡볶이, 복숭아 아이스티, 생수, 스프라이트, 아메리카노(HOT), 아...\n","화담숲주막                 [느린마을 막걸리, 병천순대, 참살이 막걸리, 찹쌀식혜, 콜라, 해물파전]\n","화담숲카페                    [메밀미숫가루, 아메리카노 HOT, 아메리카노 ICE, 현미뻥스크림]\n","Name: 메뉴명, dtype: object\n"]}]},{"cell_type":"code","source":["# 'more_2', 'more_6' 피처 정의\n","more_2_keywords = ['6인', '12인', '단체', '단체식', '플래터', '패키지', 'conference', 'Ballroom', 'Hall', '2인', '3인']\n","more_6_keywords = ['6인', '12인', '단체', '단체식', 'conference', 'Ballroom', 'Hall']\n","\n","more_2_pattern = '|'.join(more_2_keywords)\n","more_6_pattern = '|'.join(more_6_keywords)\n","\n","df_processed['more_2'] = df_processed['영업장명_메뉴명'].str.contains(more_2_pattern, case=False, na=False).astype(int)\n","df_processed['more_6'] = df_processed['영업장명_메뉴명'].str.contains(more_6_pattern, case=False, na=False).astype(int)"],"metadata":{"id":"DtFvXUORm2yb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","# 'non_menu' 피처 생성\n","bbq_non_menu = [\n","    '잔디그늘집 대여료 (6인석)', '1인 수저세트', '대여료 30,000원', '대여료 60,000원',\n","    '잔디그늘집 대여료 (12인석)', '친환경 접시 23cm', '일회용 종이컵',\n","    '잔디그늘집 의자 추가', '친환경 접시 14cm', '대여료 90,000원'\n","]\n","damha_non_menu = ['룸 이용료']\n","banquet_non_menu = [\n","    'Convention Hall', 'Grand Ballroom', 'Conference L1', 'Conference M1',\n","    'Conference L3', 'Conference L2', 'OPUS', 'Conference M9', 'Conference M8'\n","]\n","\n","# 피처 생성 함수\n","def create_non_menu(row):\n","    store = row['영업장명']\n","    menu = row['메뉴명']\n","\n","    if store == '느티나무 셀프BBQ':\n","        return 1 if menu in bbq_non_menu else 0\n","    elif store == '담하':\n","        return 1 if menu in damha_non_menu else 0\n","    elif store == '연회장':\n","        return 1 if any(non_menu_item in menu for non_menu_item in banquet_non_menu) else 0\n","    else:\n","        return 0\n","\n","# df_processed에 non_menu 피처 추가\n","df_processed['non_menu'] = df_processed.apply(create_non_menu, axis=1)"],"metadata":{"id":"I7Aq7f0Fm5Fk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","import joblib"],"metadata":{"id":"bjC_V9cbEBMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### CV"],"metadata":{"id":"tTSA-Y-Ni5Xm"}},{"cell_type":"code","source":["from itertools import product\n","import optuna\n","from optuna.samplers import TPESampler\n","from optuna.pruners import MedianPruner\n","\n","# -------------------------------\n","# 1. 데이터 분할\n","# -------------------------------\n","df_processed['영업일자'] = pd.to_datetime(df_processed['영업일자'])\n","train_cutoff = df_processed['영업일자'].max() - pd.DateOffset(days=30)\n","train_df = df_processed[df_processed['영업일자'] <= train_cutoff].copy()\n","val_df = df_processed[df_processed['영업일자'] > train_cutoff].copy()\n","\n","features = [col for col in df_processed.columns if col not in [\n","    '영업일자', '영업장명_메뉴명', '매출수량', 'year', 'month', 'weekday',\n","    'holiday_name', 'season', 'group'\n","]]\n","enc_cols = ['영업장명', '메뉴명']\n","\n","X_train, y_train = train_df[features], train_df['매출수량']\n","X_val, y_val = val_df[features], val_df['매출수량']\n","\n","# -------------------------------\n","# 2. 전처리기 (OneHotEncoder)\n","# -------------------------------\n","preprocessor = ColumnTransformer(\n","    transformers=[('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), enc_cols)],\n","    remainder='passthrough'\n",")\n","X_train_processed = preprocessor.fit_transform(X_train)\n","X_val_processed = preprocessor.transform(X_val)\n","\n","# -------------------------------\n","# 3. SMAPE 함수 정의\n","# -------------------------------\n","def smape(y_true, y_pred, eps=1e-8):\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    denom = np.maximum(np.abs(y_true) + np.abs(y_pred), eps)\n","    return np.mean(2.0 * np.abs(y_pred - y_true) / denom)\n","\n","# -------------------------------\n","# 4. 탐색할 파라미터 그리드\n","# -------------------------------\n","search_grid = {\n","    \"min_child_weight\" : [2, 3],\n","    \"max_depth\" : [10, 12],\n","    \"subsample\" : [0.8],\n","    \"learning_rate\" : [0.02, 0.05]}\n","\n","base_params = {\n","    \"objective\": \"reg:squarederror\",\n","    \"n_estimators\": 2000,\n","    \"random_state\": 42,\n","    \"n_jobs\": -1,\n","    \"tree_method\": \"hist\",\n","    \"early_stopping_rounds\": 30,\n","}\n","\n","# -------------------------------\n","# 5. 그리드 탐색 (홀드아웃)\n","# -------------------------------\n","best = {\"score\": float(\"inf\"), \"params\": None, \"best_iter\": None}\n","\n","for combo in product(*[search_grid[k] for k in search_grid]):\n","    trial_params = dict(zip(search_grid.keys(), combo))\n","    params_try = {**base_params, **trial_params, \"eval_metric\": \"rmse\"}\n","\n","    model = xgb.XGBRegressor(**params_try)\n","    model.fit(\n","        X_train_processed, y_train,\n","        eval_set=[(X_val_processed, y_val)],\n","        verbose=False\n","    )\n","\n","    best_iter = getattr(model, \"best_iteration\", None)\n","    if best_iter is None:\n","        preds = model.predict(X_val_processed)\n","    else:\n","        preds = model.predict(X_val_processed, iteration_range=(0, best_iter))\n","\n","    # 후처리\n","    preds = np.maximum(0, preds)\n","    preds = np.round(preds)\n","    preds[preds == 0] = 1\n","\n","    score = smape(y_val, preds)\n","\n","    print(\"============================\")\n","    print(f\"{trial_params}\")\n","    print(f\"{score}\")\n","\n","    if score < best[\"score\"]:\n","        best = {\"score\": score, \"params\": params_try, \"best_iter\": best_iter}\n","\n","print(\"\\n[BEST 결과]\")\n","print(\"SMAPE:\", best[\"score\"])\n","print(\"params:\", best[\"params\"])\n","print(\"best_iteration:\", best[\"best_iter\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"6aH43WvYN6fP","executionInfo":{"status":"error","timestamp":1756082859401,"user_tz":-540,"elapsed":118738,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"021a0d60-47be-4aac-fb25-07aac960f296","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================\n","{'min_child_weight': 2, 'max_depth': 10, 'subsample': 0.8, 'learning_rate': 0.02}\n","1.248736208687568\n","============================\n","{'min_child_weight': 2, 'max_depth': 10, 'subsample': 0.8, 'learning_rate': 0.05}\n","1.246785250664976\n","============================\n","{'min_child_weight': 2, 'max_depth': 12, 'subsample': 0.8, 'learning_rate': 0.02}\n","1.2472503426897315\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3399475768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams_try\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mX_train_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1248\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             _check_call(\n\u001b[0;32m-> 2247\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2248\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["! pip install optuna\n","import optuna\n","from optuna.samplers import TPESampler\n","from optuna.pruners import MedianPruner"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3_W5rCPoD-6","executionInfo":{"status":"ok","timestamp":1756081781970,"user_tz":-540,"elapsed":11671,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"d745208e-ebb8-41e2-bcba-974e9b8200b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n","Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"]}]},{"cell_type":"markdown","source":["#### 학습"],"metadata":{"id":"gqL8Nq8JlKP1"}},{"cell_type":"code","source":["def smape(y_true, y_pred):\n","    non_zero_mask = y_true != 0\n","    y_true_masked = y_true[non_zero_mask]\n","    y_pred_masked = y_pred[non_zero_mask]\n","    numerator = np.abs(y_pred_masked - y_true_masked)\n","    denominator = (np.abs(y_true_masked) + np.abs(y_pred_masked)) / 2\n","    return np.mean(numerator / denominator)\n","\n","df_processed['영업일자'] = pd.to_datetime(df_processed['영업일자'])\n","train_cutoff = df_processed['영업일자'].max() - pd.DateOffset(days=30)\n","train_df = df_processed[df_processed['영업일자'] <= train_cutoff].copy()\n","val_df = df_processed[df_processed['영업일자'] > train_cutoff].copy()\n","\n","# 4번 시나리오에 해당하는 피처 목록 정의\n","features = [col for col in df_processed.columns if col not in [\n","    '영업일자', '영업장명_메뉴명', '매출수량', 'weekday',\n","    'holiday_name', 'season'\n","]]\n","enc_cols = ['영업장명', '메뉴명']\n","\n","X_train = train_df[features]\n","y_train = train_df['매출수량']\n","X_val = val_df[features]\n","y_val = val_df['매출수량']\n","\n","# --- 2. 전처리기 및 모델 학습 ---\n","print(\"--- 4) 기존 + is_signature + is_essential 모델 실행 ---\")\n","\n","# 전처리기\n","preprocessor = ColumnTransformer(\n","    transformers=[('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), enc_cols)],\n","    remainder='passthrough'\n",")\n","X_train_processed = preprocessor.fit_transform(X_train)\n","X_val_processed = preprocessor.transform(X_val)\n","\n","# XGBoost 파라미터\n","params = {\n","    'objective': 'reg:squarederror', 'n_estimators': 2000,\n","    'learning_rate': 0.02, 'max_depth': 10, 'subsample': 0.9,\n","    'colsample_bytree': 0.8, 'early_stopping_rounds': 30,\n","    'random_state': 42, 'n_jobs': -1\n","}\n","\n","# 모델 학습\n","xgb_model = xgb.XGBRegressor(**params)\n","xgb_model.fit(\n","    X_train_processed, y_train,\n","    eval_set=[(X_val_processed, y_val)],\n","    verbose=False\n",")\n","\n","# --- 3. 예측 및 최종 SMAPE 점수 출력 ---\n","best_iteration = xgb_model.best_iteration\n","predictions = xgb_model.predict(X_val_processed, iteration_range=(0, best_iteration))\n","predictions = np.round(np.maximum(0, predictions))\n","predictions[predictions == 0] = 1\n","\n","smape_score = smape(y_val, predictions)\n","\n","print(f\"\\n 최종 Validation SMAPE: {smape_score:.4f}\")\n","print(f\"   (최적 나무 개수: {best_iteration})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kolmlI_DD5YZ","executionInfo":{"status":"ok","timestamp":1756083073956,"user_tz":-540,"elapsed":43080,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"34512d78-c34c-4421-ec4e-bc0fa1d53b54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 4) 기존 + is_signature + is_essential 모델 실행 ---\n","\n"," 최종 Validation SMAPE: 0.5193\n","   (최적 나무 개수: 528)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","import joblib\n","\n","# 사용할 피처 및 전처리기 정의\n","enc_cols = ['영업장명', '메뉴명']\n","features = [col for col in df_processed.columns if col not in [\n","    '영업일자', '영업장명_메뉴명', '매출수량', 'weekday',\n","    'holiday_name', 'season'\n","]]\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), enc_cols)\n","    ],\n","    remainder='passthrough'\n",")\n","\n","#  전체 데이터로 최종 모델 학습 및 저장\n","X_full = df_processed[features]\n","y_full = df_processed['매출수량']\n","\n","print(\"최종 모델 학습 시작...\")\n","X_full_processed = preprocessor.fit_transform(X_full)\n","\n","# 가장 성능이 좋았던 파라미터로 최종 모델 정의\n","final_model = xgb.XGBRegressor(\n","    objective='reg:squarederror',\n","    n_estimators=528, ################################\n","    learning_rate=0.02,\n","    max_depth=10,\n","    subsample=0.9,\n","    colsample_bytree=0.8,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","final_model.fit(X_full_processed, y_full)\n","\n","joblib.dump(final_model, '/content/drive/MyDrive/3. Grad School/LG Aimers/Models/Trial 11/final_xgb_model.pkl') ################################\n","joblib.dump(preprocessor, '/content/drive/MyDrive/3. Grad School/LG Aimers/Models/Trial 11/final_xgb_preprocessor.pkl') ################################\n","\n","print(\"\\n최종 모델과 전처리기 저장 완료\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srYlnE2YElZu","executionInfo":{"status":"ok","timestamp":1756083129219,"user_tz":-540,"elapsed":41852,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"ec5c679c-0c37-47d7-da83-2695fef03bb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["최종 모델 학습 시작...\n","\n","최종 모델과 전처리기 저장 완료\n"]}]},{"cell_type":"code","source":["model = joblib.load('/content/drive/MyDrive/3. Grad School/LG Aimers/Models/Trial 11/final_xgb_model.pkl') ################################"],"metadata":{"id":"HoDTFkL9hMLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 예측"],"metadata":{"id":"1Gf8QXtJhORH"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","import joblib\n","import glob\n","import os\n","import re\n","\n","# --- 1. 저장된 모델, 전처리기, 데이터 로드 ---\n","model = joblib.load('/content/drive/MyDrive/3. Grad School/LG Aimers/Models/Trial 11/final_xgb_model.pkl') ################################\n","preprocessor = joblib.load('/content/drive/MyDrive/3. Grad School/LG Aimers/Models/Trial 11/final_xgb_preprocessor.pkl') ################################\n","sample_submission = pd.read_csv('/content/drive/MyDrive/3. Grad School/LG Aimers/baseline_submission.csv') ################################\n","train_df_for_rules = data\n","mv = Make_Variables()\n","print(\"모델 및 전처리기 로드 완료\")\n","\n","# --- 2. 예측에 필요한 규칙/정보 사전 계산 ---\n","sales_sum = train_df_for_rules.groupby('영업장명_메뉴명')['매출수량'].sum()\n","signature_menus = sales_sum[sales_sum >= 10000].index.tolist()\n","features_to_use = preprocessor.feature_names_in_\n","\n","# --- 3. 테스트 파일 순회하며 예측 수행 ---\n","all_predictions_long = []\n","test_files = sorted(glob.glob('DATA/test/TEST_*.csv'))\n","print(f\"\\n{len(test_files)}개 테스트 파일 예측 시작...\")\n","\n","for path in test_files:\n","    test_df = pd.read_csv(path)\n","    test_df['영업일자'] = pd.to_datetime(test_df['영업일자'])\n","\n","    filename = os.path.basename(path)\n","    test_prefix = re.search(r'(TEST_\\d+)', filename).group(1)\n","\n","    # 기본 피처 및 추가 피처 생성\n","    last_date = test_df['영업일자'].max()\n","    future_df_full = mv.make_variables_test(date=last_date, test_df=test_df, original_data = df_processed, predict=7)\n","    future_df_full.fillna(0, inplace=True)\n","    future_df_full['is_signature'] = future_df_full['영업장명_메뉴명'].isin(signature_menus).astype(int)\n","    future_df_full['is_essential'] = future_df_full.apply(create_is_essential, axis=1)\n","\n","    for menu, menu_future_df in future_df_full.groupby('영업장명_메뉴명'):\n","\n","        X_test = menu_future_df[features_to_use]\n","        X_test_processed = preprocessor.transform(X_test)\n","        predictions = model.predict(X_test_processed)\n","\n","        # 후처리\n","        predictions = np.maximum(0, predictions)\n","        predictions = np.round(predictions)\n","        predictions[predictions == 0] = 1\n","\n","        # 결과 저장\n","        pred_dates = [f\"{test_prefix}+{i+1}일\" for i in range(7)]\n","        result_df = pd.DataFrame({\n","            '영업일자': pred_dates,\n","            '영업장명_메뉴명': menu,\n","            '매출수량': predictions\n","        })\n","        all_predictions_long.append(result_df)\n","\n","    print(f\" - {filename} 예측 완료.\")\n","\n","def convert_to_submission_format(pred_df: pd.DataFrame, sample_submission: pd.DataFrame):\n","    pred_dict = dict(zip(zip(pred_df['영업일자'], pred_df['영업장명_메뉴명']), pred_df['매출수량']))\n","    final_df = sample_submission.copy()\n","    menu_cols = final_df.columns[1:]\n","    final_df[menu_cols] = final_df[menu_cols].astype(float)\n","\n","    for row_idx in final_df.index:\n","        date = final_df.loc[row_idx, '영업일자']\n","        for col in menu_cols:\n","            final_df.loc[row_idx, col] = pred_dict.get((date, col), 0) # 예측값이 없으면 0으로 채움\n","    return final_df\n","\n","# --- 4. 최종 제출 파일 생성 ---\n","final_pred_df_long = pd.concat(all_predictions_long, ignore_index=True)\n","submission_df5 = convert_to_submission_format(final_pred_df_long, sample_submission)\n","submission_df5.to_csv('submission5.csv', index=False)\n","\n","print(\"\\n예측 완료. 'submission5.csv' 파일이 생성되었습니다.\")\n","print(\"제출 파일 미리보기:\")\n","print(submission_df5.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8UZhvaUFVaJ","executionInfo":{"status":"ok","timestamp":1756083188460,"user_tz":-540,"elapsed":48239,"user":{"displayName":"박준희","userId":"01167826914931997240"}},"outputId":"beff9ddc-0157-414f-adde-fa9b7d297862"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["모델 및 전처리기 로드 완료\n","\n","10개 테스트 파일 예측 시작...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_00.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_01.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_02.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_03.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_04.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_05.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_06.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_07.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_08.csv 예측 완료.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2085181274.py:37: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  future_df_full.fillna(0, inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":[" - TEST_09.csv 예측 완료.\n","\n","예측 완료. 'submission5.csv' 파일이 생성되었습니다.\n","제출 파일 미리보기:\n","         영업일자  느티나무 셀프BBQ_1인 수저세트  느티나무 셀프BBQ_BBQ55(단체)  \\\n","0  TEST_00+1일                11.0                   6.0   \n","1  TEST_00+2일                 5.0                  41.0   \n","2  TEST_00+3일                 5.0                  77.0   \n","3  TEST_00+4일                 5.0                  40.0   \n","4  TEST_00+5일                 7.0                  90.0   \n","\n","   느티나무 셀프BBQ_대여료 30,000원  느티나무 셀프BBQ_대여료 60,000원  느티나무 셀프BBQ_대여료 90,000원  \\\n","0                     7.0                     3.0                     1.0   \n","1                     3.0                     1.0                     1.0   \n","2                     3.0                     2.0                     1.0   \n","3                     3.0                     1.0                     1.0   \n","4                     4.0                     2.0                     1.0   \n","\n","   느티나무 셀프BBQ_본삼겹 (단품,실내)  느티나무 셀프BBQ_스프라이트 (단체)  느티나무 셀프BBQ_신라면  \\\n","0                     1.0                    1.0             3.0   \n","1                     1.0                    5.0             2.0   \n","2                     1.0                    5.0             2.0   \n","3                     1.0                    6.0             2.0   \n","4                     1.0                   13.0             2.0   \n","\n","   느티나무 셀프BBQ_쌈야채세트  ...  화담숲주막_스프라이트  화담숲주막_참살이 막걸리  화담숲주막_찹쌀식혜  화담숲주막_콜라  \\\n","0               2.0  ...          8.0           16.0        16.0       7.0   \n","1               1.0  ...          1.0            3.0         3.0       1.0   \n","2               1.0  ...          2.0            7.0         6.0       2.0   \n","3               1.0  ...          2.0            6.0         5.0       2.0   \n","4               2.0  ...          3.0            6.0         5.0       3.0   \n","\n","   화담숲주막_해물파전  화담숲카페_메밀미숫가루  화담숲카페_아메리카노 HOT  화담숲카페_아메리카노 ICE  화담숲카페_카페라떼 ICE  \\\n","0        60.0          32.0              5.0             40.0            10.0   \n","1         1.0           2.0              1.0              5.0             2.0   \n","2        32.0          22.0              3.0             27.0             4.0   \n","3        23.0          15.0              2.0             16.0             4.0   \n","4        25.0          18.0              3.0             21.0             5.0   \n","\n","   화담숲카페_현미뻥스크림  \n","0          17.0  \n","1           2.0  \n","2           7.0  \n","3           7.0  \n","4           6.0  \n","\n","[5 rows x 194 columns]\n"]}]}]}