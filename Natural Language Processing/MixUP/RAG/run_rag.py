import os
import sys
import argparse
import importlib.util
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# Load environment variables
load_dotenv()

# ==========================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í”„ë¡¬í”„íŠ¸ ë¡œë“œ)
# ==========================================
def load_dynamic_prompt(file_path, var_names=["system_prompt", "baseline_prompt"]):
    """ì§€ì •ëœ ê²½ë¡œì˜ íŒŒì´ì¬ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ë¥¼ ë¡œë“œ"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    abs_path = os.path.abspath(file_path)
    module_name = os.path.basename(file_path).replace(".py", "")

    spec = importlib.util.spec_from_file_location(module_name, abs_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    for var_name in var_names:
        if hasattr(module, var_name):
            print(f"âœ… Loaded '{var_name}' from {os.path.basename(file_path)}")
            return getattr(module, var_name)
    
    raise ValueError(f"íŒŒì¼ {file_path} ì•ˆì— ë³€ìˆ˜({var_names})ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ==========================================
# 2. ì „ì—­ ì„¤ì • (Upstage, ChromaDB)
# ==========================================
api_key = os.getenv("UPSTAGE_API_KEY")
if not api_key:
    raise ValueError("UPSTAGE_API_KEY not found in .env file")

client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")

print("Loading Embedding Model (BAAI/bge-m3)...")
embedding_function = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'}, 
    encode_kwargs={'normalize_embeddings': True}
)

print("Connecting to Vector DB...")
vectorstore = Chroma(
    persist_directory="./chroma_db_bge", 
    collection_name="translation_memory",
    embedding_function=embedding_function
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ==========================================
# 3. í•µì‹¬ ì‘ì—… í•¨ìˆ˜ (RAG + Generation)
# ==========================================
def generate_with_rag(text, model_name, system_prompt_text):
    try:
        # [Step 1] ìœ ì‚¬ ì˜ˆì‹œ ê²€ìƒ‰
        docs = retriever.invoke(text)
        
        # [Step 2] Few-shot ì˜ˆì‹œ í…ìŠ¤íŠ¸ êµ¬ì„±
        examples_text = ""
        for i, doc in enumerate(docs, 1):
            examples_text += f"ì°¸ê³  ì˜ˆì‹œ {i}:\n[ì›ë¬¸] {doc.page_content}\n[ë³€í™˜] {doc.metadata['answer']}\n\n"
            
        # [Step 3] ìœ ì € í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # ì™¸ë¶€ì—ì„œ ë¶ˆëŸ¬ì˜¨ system_promptëŠ” 'ê·œì¹™' ì—­í• ì„ í•˜ê³ ,
        # ì—¬ê¸°ì„œëŠ” 'ì˜ˆì‹œ'ì™€ 'ì…ë ¥'ì„ ì œê³µí•©ë‹ˆë‹¤.
        user_prompt = f"""
ë‹¤ìŒì€ ë‹¹ì‹ ì´ ì°¸ê³ í•´ì•¼ í•  [ìœ ì‚¬ ë³€í™˜ ì˜ˆì‹œ]ì…ë‹ˆë‹¤.
ì´ ì˜ˆì‹œë“¤ì˜ ë¬¸ì²´ì™€ ë‹¨ì–´ ì„ íƒì„ ì°¸ê³ í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

{examples_text}
--------------------------------------------------
[ì²˜ë¦¬í•  ì›ë¬¸]
{text}
"""

        # [Step 4] API í˜¸ì¶œ
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                # [í•µì‹¬] íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¨ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ê¸°ì— ì£¼ì…
                {"role": "system", "content": system_prompt_text},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
        )
        
        return resp.choices[0].message.content.strip()

    except Exception as e:
        print(f"[ERROR] {text[:30]}... - {e}")
        return text

# ==========================================
# 4. ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================
def main():
    parser = argparse.ArgumentParser(description="Generate with RAG using Solar-pro2")
    parser.add_argument("--input", default="data/train_sampled.csv", help="Input CSV path")
    parser.add_argument("--output", default="submissions/submission_rag_sampled.csv", help="Output CSV path")
    parser.add_argument("--model", default="solar-pro2", help="Model name")
    
    # [ì¶”ê°€] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê²½ë¡œ ì˜µì…˜
    parser.add_argument("--system_prompt_path", default="prompts/prompt_5.py", help="Path to system prompt file")
    
    args = parser.parse_args()

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(args.input)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    print(f"ğŸ“‚ Loading system prompt from: {args.system_prompt_path}")
    try:
        loaded_system_prompt = load_dynamic_prompt(args.system_prompt_path)
    except Exception as e:
        print(f"âŒ Error loading prompt: {e}")
        return

    print(f"Model: {args.model}")
    print(f"Input: {args.input} ({len(df)} rows)")
    print(f"System Prompt Preview: {loaded_system_prompt[:50]}...")
    
    results = {idx: None for idx in range(len(df))}
    max_workers = 4
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # generate_with_rag í•¨ìˆ˜ì— loaded_system_promptë¥¼ í•¨ê»˜ ì „ë‹¬
        futures = {
            executor.submit(generate_with_rag, text, args.model, loaded_system_prompt): idx
            for idx, text in enumerate(df["original_sentence"].astype(str).tolist())
        }

        for future in tqdm(as_completed(futures), total=len(futures), desc="RAG Generating"):
            idx = futures[future]
            results[idx] = future.result()

    final_results = []
    for idx in range(len(df)):
        final_results.append({
            "id": df.iloc[idx]["id"],
            "original_sentence": df.iloc[idx]["original_sentence"],
            "answer_sentence": results[idx] if results[idx] is not None else df.iloc[idx]["original_sentence"]
        })
    
    out_df = pd.DataFrame(final_results)
    out_df.to_csv(args.output, index=False)
    print(f"âœ… Saved to {args.output}")

if __name__ == "__main__":
    main()