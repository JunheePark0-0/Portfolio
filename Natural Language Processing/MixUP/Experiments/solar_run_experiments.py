import os
import sys
import subprocess
import importlib.util
import pandas as pd
import glob
from datetime import datetime
import argparse


def load_prompt_from_file(file_path):
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ì—ì„œ baseline_promptë¥¼ ë¡œë“œ"""
    spec = importlib.util.spec_from_file_location("prompt_module", file_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    
    if not hasattr(module, "baseline_prompt"):
        raise ValueError(f"File {file_path} does not contain 'baseline_prompt'")
    
    return module.baseline_prompt


def run_generate(prompt_file, input_file, output_file, model="solar-pro2"):
    """baseline_generate_multi.py ì‹¤í–‰"""
    prompt = load_prompt_from_file(prompt_file)
    
    cmd = [
        sys.executable,
        "-u",
        "baseline_generate_multi.py",
        "--input", input_file,
        "--output", output_file,
        "--model", model,
        "--prompt", prompt
    ]
    
    print(f"\n{'='*60}")
    print(f"ğŸ”„ Generating with prompt: {os.path.basename(prompt_file)}")
    print(f"{'='*60}")
    
    # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ capture_output=Falseë¡œ ì„¤ì •
    # stdoutê³¼ stderrë¥¼ ì§ì ‘ ì—°ê²°í•˜ì—¬ tqdmì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì´ë„ë¡ í•¨
    result = subprocess.run(cmd)
    
    if result.returncode != 0:
        print(f"\nâŒ Generation failed with return code {result.returncode}")
        return False
    
    return True


def run_evaluate(true_df_path, pred_df_path):
    """evaluate.py ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜"""
    from evaluate import evaluate
    import pandas as pd
    
    true_df = pd.read_csv(true_df_path)
    pred_df = pd.read_csv(pred_df_path)
    
    # evaluate í•¨ìˆ˜ê°€ ë‚´ë¶€ì—ì„œ metrics.evaluate_correctionì„ í˜¸ì¶œí•˜ë¯€ë¡œ
    # í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ ì¬ì‚¬ìš©
    result_df, summary_text = evaluate(true_df, pred_df)
    
    # average_scoresëŠ” evaluate í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ê³„ì‚°ë˜ì§€ë§Œ ë°˜í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ
    # ë‹¤ì‹œ ê³„ì‚° (ë˜ëŠ” evaluate í•¨ìˆ˜ ìˆ˜ì • í•„ìš”)
    import metrics
    _, average_scores = metrics.evaluate_correction(true_df, pred_df)
    
    return average_scores, summary_text


def main():
    parser = argparse.ArgumentParser(description="Run experiments with all prompts")
    parser.add_argument("--input", default="data/train_dataset.csv", help="Input CSV path")
    parser.add_argument("--prompt_dir", default="prompt", help="Directory containing prompt files")
    parser.add_argument("--output_dir", default="experiments", help="Output directory for experiment results")
    parser.add_argument("--model", default="solar-pro2", help="Model name")
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs("submissions", exist_ok=True)
    
    # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì°¾ê¸°
    prompt_files = sorted(glob.glob(os.path.join(args.prompt_dir, "*.py")))
    
    if not prompt_files:
        print(f"âŒ No prompt files found in {args.prompt_dir}")
        return
    
    print(f"ğŸ“‹ Found {len(prompt_files)} prompt files")
    
    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    experiment_results = []
    
    for i, prompt_file in enumerate(prompt_files, 1):
        prompt_name = os.path.basename(prompt_file).replace(".py", "")
        temp_output = f"submissions/submission_{prompt_name}.csv"
        
        print(f"\n{'#'*60}")
        print(f"# Experiment {i}/{len(prompt_files)}: {prompt_name}")
        print(f"{'#'*60}")
        
        try:
            # 1. Generate
            if not run_generate(prompt_file, args.input, temp_output, args.model):
                print(f"âŒ Skipping {prompt_name} due to generation failure")
                continue
            
            # 2. Evaluate
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Evaluating: {prompt_name}")
            print(f"{'='*60}")
            
            average_scores, summary_text = run_evaluate(args.input, temp_output)
            
            # 3. ê²°ê³¼ ì €ì¥
            result_row = {
                "prompt_file": prompt_name,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "model": args.model,
            }
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ì¶”ê°€
            for category, stats in average_scores.items():
                if category == "overall":
                    result_row["overall_score"] = stats.get("average_score", 0.0)
                else:
                    result_row[f"{category}_score"] = stats.get("average_score", 0.0)
            
            experiment_results.append(result_row)
            
            # ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì €ì¥ (ì¤‘ê°„ ì €ì¥)
            if experiment_results:
                exp_df = pd.DataFrame(experiment_results)
                intermediate_path = os.path.join(args.output_dir, f"experiment_{args.prompt_dir}.csv")
                exp_df.to_csv(intermediate_path, index=False, encoding="utf-8-sig")
                print(f"\nğŸ’¾ Experiment results saved to {intermediate_path}")
            
            print(f"\nâœ… Completed: {prompt_name}")
            print(f"   Overall Score: {result_row.get('overall_score', 'N/A')}")
            
        except Exception as e:
            print(f"âŒ Error processing {prompt_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    if experiment_results:
        final_output_path = os.path.join(args.output_dir, f"experiment_{args.prompt_dir}.csv")
        exp_df = pd.DataFrame(experiment_results)
        exp_df.to_csv(final_output_path, index=False, encoding="utf-8-sig")
        print(f"\n{'='*60}")
        print(f"âœ… All experiments completed!")
        print(f"ğŸ“Š Results saved to {final_output_path}")
        print(f"{'='*60}")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if "overall_score" in exp_df.columns:
            best_idx = exp_df["overall_score"].idxmax()
            best_prompt = exp_df.loc[best_idx, "prompt_file"]
            best_score = exp_df.loc[best_idx, "overall_score"]
            print(f"\nğŸ† Best prompt: {best_prompt} (score: {best_score:.3f})")
    else:
        print("\nâŒ No experiments completed successfully")


if __name__ == "__main__":
    main()

